{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4ffP2tMgWj729eJtZq4Wz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crypto-saac/test_demo/blob/master/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEGz-aE4_rYg",
        "outputId": "fa9e50cd-0a11-47c5-eb3f-4d3f25f2ed6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from IPython.display import SVG\n",
        "#from keras.layers.vis_utils import model_to_dot\n"
      ],
      "metadata": {
        "id": "9d9rcqmMDVW-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filePath='/content/drive/MyDrive/training.1600000.processed.noemoticon.csv'"
      ],
      "metadata": {
        "id": "z9E7sbjhEZiP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "df = pd.read_csv(filePath, header=0,encoding='latin-1', names=columns)\n"
      ],
      "metadata": {
        "id": "pm820crHER0n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myData= df[[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]]\n",
        "myData['flag']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZpz-DB1HfRb",
        "outputId": "cbc7ad9f-bc73-436a-d870-943a47d7950b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          NO_QUERY\n",
              "1          NO_QUERY\n",
              "2          NO_QUERY\n",
              "3          NO_QUERY\n",
              "4          NO_QUERY\n",
              "             ...   \n",
              "1599994    NO_QUERY\n",
              "1599995    NO_QUERY\n",
              "1599996    NO_QUERY\n",
              "1599997    NO_QUERY\n",
              "1599998    NO_QUERY\n",
              "Name: flag, Length: 1599999, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
        "VAL_SIZE = 1000  # Size of the validation set\n",
        "NB_START_EPOCHS = 10  # Number of epochs we usually start to train with\n",
        "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
        "MAX_LEN = 24  # Maximum number of words in a sequence\n",
        "GLOVE_DIM = 100  # Number of dimensions of the GloVe word embeddings"
      ],
      "metadata": {
        "id": "IiWYy6T3LAZg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = Path('../')\n",
        "input_path = root / 'input/' \n",
        "ouput_path = root / 'output/'\n",
        "source_path = root / 'source/'\n"
      ],
      "metadata": {
        "id": "Z_fYPHbb1uzu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
        "    '''\n",
        "    Function to train a multi-class model. The number of epochs and \n",
        "    batch_size are set by the constants at the top of the\n",
        "    notebook. \n",
        "    \n",
        "    Parameters:\n",
        "        model : model with the chosen architecture\n",
        "        X_train : training features\n",
        "        y_train : training target\n",
        "        X_valid : validation features\n",
        "        Y_valid : validation target\n",
        "    Output:\n",
        "        model training history\n",
        "    '''\n",
        "    model.compile(optimizer='rmsprop'\n",
        "                  , loss='categorical_crossentropy'\n",
        "                  , metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(X_train\n",
        "                       , y_train\n",
        "                       , epochs=NB_START_EPOCHS\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=0)\n",
        "    return history\n",
        "\n",
        "def eval_metric(history, metric_name):\n",
        "    '''\n",
        "    \n",
        "    Function to evaluate a trained model on a chosen metric. \n",
        "    Training and validation metric are plotted in a\n",
        "    line chart for each epoch.\n",
        "    \n",
        "    Parameters:\n",
        "        history : model training history\n",
        "        metric_name : loss or accuracy\n",
        "    Output:\n",
        "        line chart with epochs of x-axis and metric on\n",
        "        y-axis\n",
        "    '''\n",
        "    metric = history.history[metric_name]\n",
        "    val_metric = history.history['val_' + metric_name]\n",
        "\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "\n",
        "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
        "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
        "    '''\n",
        "    Function to test the model on new data after training it\n",
        "    on the full training data with the optimal number of epochs.\n",
        "    \n",
        "    Parameters:\n",
        "        model : trained model\n",
        "        X_train : training features\n",
        "        y_train : training target\n",
        "        X_test : test features\n",
        "        y_test : test target\n",
        "        epochs : optimal number of epochs\n",
        "    Output:\n",
        "        test accuracy and test loss\n",
        "    '''\n",
        "    model.fit(X_train\n",
        "              , y_train\n",
        "              , epochs=epoch_stop\n",
        "              , batch_size=BATCH_SIZE\n",
        "              , verbose=0)\n",
        "    results = model.evaluate(X_test, y_test)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def remove_stopwords(input_text):\n",
        "    '''\n",
        "    Function to remove English stopwords from a Pandas Series.\n",
        "    \n",
        "    Parameters:\n",
        "        input_text : text to clean\n",
        "    Output:\n",
        "        cleaned Pandas Series \n",
        "    '''\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    words = input_text.split() \n",
        "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "    return \" \".join(clean_words) \n",
        "    \n",
        "def remove_mentions(input_text):\n",
        "    '''\n",
        "    Function to remove mentions, preceded by @, in a Pandas Series\n",
        "    \n",
        "    Parameters:\n",
        "        input_text : text to clean\n",
        "    Output:\n",
        "        cleaned Pandas Series \n",
        "    '''\n",
        "    return re.sub(r'@\\w+', '', input_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZNXJlGMY42r"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading and cleaning data\n"
      ],
      "metadata": {
        "id": "LdYzdaYcZfAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "df = pd.read_csv(filePath, header=0,encoding='latin-1', names=columns)\n",
        "df = df.reindex(np.random.permutation(df.index))  \n",
        "df = df[['target', 'text']]\n",
        "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iylo-hDEY7AC",
        "outputId": "6f6da203-008d-49bf-8915-8034c9c0a4f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIZMGsMEbFLg",
        "outputId": "3fed2c5a-110f-4bfc-8376-92ff252c7887"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "298777          Am reading book review can't play blip 2nite\n",
            "542094                                                      \n",
            "1405747                      like name way is...Tom Fletcher\n",
            "683332                                         die food coma\n",
            "461215           lol yea not hard dont know alot pep wit one\n",
            "                                 ...                        \n",
            "969648                   Work- getting belly button pierced \n",
            "1335809                                                     \n",
            "99103      went trying grab new Lightning pics mornings s...\n",
            "624016                          #Silkroad server full, play?\n",
            "865480                                Thank's recommendation\n",
            "Name: text, Length: 1599999, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text,df.target, test_size=0.1, random_state=37)\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO-s1mund5m8",
        "outputId": "a506b3b5-6921-45a5-e00e-7453ea217b84"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Train data samples: 1439999\n",
            "# Test data samples: 160000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "id": "qetqwcQP3oMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(nb_words=NB_WORDS)\n",
        "print(tk)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yy53r0vwhr1",
        "outputId": "6bb1f62e-dc45-4254-dfd9-7ca5792e5ed5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.preprocessing.text.Tokenizer object at 0x7fc4ab543430>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/text.py:246: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "xQilMttO4oor"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
        "seq_lengths.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMb2p8BT6JL2",
        "outputId": "eae6d88b-bcf5-4175-8024-808eca988a3f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1.439999e+06\n",
              "mean     8.341832e+00\n",
              "std      4.247361e+00\n",
              "min      1.000000e+00\n",
              "25%      5.000000e+00\n",
              "50%      8.000000e+00\n",
              "75%      1.100000e+01\n",
              "max      4.100000e+01\n",
              "Name: text, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the figures above we will set MAX_LEN to 24. So this means we will not be truncating any words, only pad with zeros. This is to avoid to lose information as the tweets are rather short."
      ],
      "metadata": {
        "id": "aqqfMnkL6Vzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
        "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)\n"
      ],
      "metadata": {
        "id": "YIV22Pbu6cnH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_seq_trunc[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWjs4iqQ6oKY",
        "outputId": "23e21942-b29f-4a55-bd4c-9e78fd4b3a23"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1657,\n",
              "         10,   22], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)"
      ],
      "metadata": {
        "id": "7JCC00pV6upA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)\n",
        "\n",
        "assert X_valid_emb.shape[0] == y_valid_emb.shape[0]\n",
        "assert X_train_emb.shape[0] == y_train_emb.shape[0]\n",
        "\n",
        "print('Shape of validation set:',X_valid_emb.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY0xSvHV61vo",
        "outputId": "5f59ee7a-451e-43fa-e68e-105b272ce000"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of validation set: (144000, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_model = models.Sequential()\n",
        "emb_model.add(layers.Embedding(NB_WORDS, 8, input_length=MAX_LEN))\n",
        "emb_model.add(layers.Flatten())\n",
        "emb_model.add(layers.Dense(3, activation='softmax'))\n",
        "emb_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQAXahEt6-9B",
        "outputId": "51959058-2a0b-4631-ba51-dd75ed1a7604"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 24, 8)             80000     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 192)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 579       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,579\n",
            "Trainable params: 80,579\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)\n",
        "emb_history.history['acc'][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "apd9f_Lo7QQR",
        "outputId": "b48154d7-c071-4eb6-a8bc-157d2e36270b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-227f2770e94c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0memb_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-b7f9adfe1a12>\u001b[0m in \u001b[0;36mdeep_model\u001b[0;34m(model, X_train, y_train, X_valid, y_valid)\u001b[0m\n\u001b[1;32m     18\u001b[0m                   , metrics=['accuracy'])\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     history = model.fit(X_train\n\u001b[0m\u001b[1;32m     21\u001b[0m                        \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                        \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNB_START_EPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 3) are incompatible\n"
          ]
        }
      ]
    }
  ]
}